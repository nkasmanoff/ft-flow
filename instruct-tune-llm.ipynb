{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruct tuning the model\n",
    "\n",
    "This notebook draws heavily a similar one done for the [phi3](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/sample_finetune.py) model. \n",
    "\n",
    "The difference here is that this will focus on a model's full fine-tuning process, work for going from a base model to a new insruction model, and should work for almost any model on HuggingFace.\n",
    "\n",
    "At the end of the notebook are the steps to save this as a gguf format which will allow for fast and easy inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "import os\n",
    "import json\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnoahpunintended\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20241113_122447-9fku9hau</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/noahpunintended/smollm-ft/runs/9fku9hau' target=\"_blank\">eager-mountain-40</a></strong> to <a href='https://wandb.ai/noahpunintended/smollm-ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/noahpunintended/smollm-ft' target=\"_blank\">https://wandb.ai/noahpunintended/smollm-ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/noahpunintended/smollm-ft/runs/9fku9hau' target=\"_blank\">https://wandb.ai/noahpunintended/smollm-ft/runs/9fku9hau</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "wandb.init(project=\"smollm-ft\")\n",
    "###################\n",
    "# Hyper-parameters\n",
    "###################\n",
    "training_config = {\n",
    "    \"do_eval\": False,\n",
    "    \"learning_rate\": 2.0e-4,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"per_device_train_batch_size\": 8,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"log_level\": \"info\",\n",
    "    \"logging_steps\": 150,\n",
    "    \"logging_strategy\": \"steps\",\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"num_train_epochs\": 8,\n",
    "    \"max_steps\": -1,\n",
    "    \"output_dir\": \"./picard-ft\",\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"remove_unused_columns\": True,\n",
    "    \"save_steps\": 500,\n",
    "    \"save_total_limit\": 1,\n",
    "    \"seed\": 0,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"gradient_checkpointing_kwargs\":{\"use_reentrant\": False},\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"warmup_ratio\": 0.05,\n",
    "    \"report_to\":\"wandb\",\n",
    "    \"neftune_noise_alpha\":5,\n",
    "    \"push_to_hub\": True,\n",
    "    }\n",
    "\n",
    "train_conf = TrainingArguments(**training_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-13 12:25:06 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=no,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs={'use_reentrant': False},\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0002,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./picard-ft/runs/Nov13_12-24-50_ip-10-192-12-163,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=150,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=5,\n",
      "no_cuda=False,\n",
      "num_train_epochs=8,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./picard-ft,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=True,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./picard-ft,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=0,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.001,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# Setup logging\n",
    "###############\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "log_level = train_conf.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "logger.info(f\"Training/evaluation parameters {train_conf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-11-13 12:25:09,201 >> loading configuration file config.json from cache at /home/zeus/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M/snapshots/3ce05f63c246c44616da500b47b01f082f4d3bcc/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-11-13 12:25:09,204 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"HuggingFaceTB/SmolLM2-360M\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 960,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2560,\n",
      "  \"is_llama_config\": true,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 15,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 5,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_interleaved\": false,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 49152\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3678] 2024-11-13 12:25:09,287 >> loading weights file model.safetensors from cache at /home/zeus/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M/snapshots/3ce05f63c246c44616da500b47b01f082f4d3bcc/model.safetensors\n",
      "[INFO|modeling_utils.py:1606] 2024-11-13 12:25:09,385 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1038] 2024-11-13 12:25:09,388 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4507] 2024-11-13 12:25:10,017 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4515] 2024-11-13 12:25:10,018 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at HuggingFaceTB/SmolLM2-360M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:993] 2024-11-13 12:25:10,072 >> loading configuration file generation_config.json from cache at /home/zeus/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M/snapshots/3ce05f63c246c44616da500b47b01f082f4d3bcc/generation_config.json\n",
      "[INFO|configuration_utils.py:1038] 2024-11-13 12:25:10,073 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-11-13 12:25:10,234 >> loading file vocab.json from cache at /home/zeus/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M-Instruct/snapshots/4873f67095301d304753fae05bc09ec766634e50/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-11-13 12:25:10,236 >> loading file merges.txt from cache at /home/zeus/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M-Instruct/snapshots/4873f67095301d304753fae05bc09ec766634e50/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-11-13 12:25:10,236 >> loading file tokenizer.json from cache at /home/zeus/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M-Instruct/snapshots/4873f67095301d304753fae05bc09ec766634e50/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-11-13 12:25:10,237 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-11-13 12:25:10,237 >> loading file special_tokens_map.json from cache at /home/zeus/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M-Instruct/snapshots/4873f67095301d304753fae05bc09ec766634e50/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-11-13 12:25:10,237 >> loading file tokenizer_config.json from cache at /home/zeus/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M-Instruct/snapshots/4873f67095301d304753fae05bc09ec766634e50/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "####################\n",
    "# Base Model Loading\n",
    "####################\n",
    "checkpoint_path = \"HuggingFaceTB/SmolLM2-360M\"\n",
    "model_kwargs = dict(\n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    "#    attn_implementation=\"flash_attention_2\",  # only works on latest gpus, probably not worth it in most cases\n",
    "     torch_dtype=torch.bfloat16,\n",
    "   device_map='auto'\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_path, **model_kwargs)\n",
    "\n",
    "\n",
    "###################\n",
    "# Tokenizer Loading\n",
    "###################\n",
    "\n",
    "checkpoint_path = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "tokenizer.model_max_length = 2048\n",
    "tokenizer.pad_token = \"<|endoftext|>\"  # note this is specific to smollm\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "tokenizer.padding_side = 'right'\n",
    "# https://stackoverflow.com/questions/76446228/setting-padding-token-as-eos-token-when-using-datacollatorforlanguagemodeling-fr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|endoftext|>', '<|im_end|>', 2, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token, tokenizer.eos_token, tokenizer.eos_token_id, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the fine-tune \n",
    "\n",
    "Now that the synthetic dataset is made, next up is ensure the model is capable of answering like we expect, without the large system prompt impacting latency. \n",
    "\n",
    "The solution to this is to open up the dataset, replace the system prompt with something much simpler, and starting training with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-13 12:25:16 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-13 12:25:16 - INFO - datasets.info - Loading Dataset info from /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset pi-card-sft-data (/home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-13 12:25:16 - INFO - datasets.builder - Found cached dataset pi-card-sft-data (/home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-13 12:25:16 - INFO - datasets.info - Loading Dataset info from /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #0 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-7b2331d0b396f378_00000_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-13 12:25:17 - INFO - datasets.arrow_dataset - Process #0 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-7b2331d0b396f378_00000_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #1 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-7b2331d0b396f378_00001_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-13 12:25:17 - INFO - datasets.arrow_dataset - Process #1 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-7b2331d0b396f378_00001_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #2 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-7b2331d0b396f378_00002_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-13 12:25:17 - INFO - datasets.arrow_dataset - Process #2 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-7b2331d0b396f378_00002_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #3 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-7b2331d0b396f378_00003_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-13 12:25:17 - INFO - datasets.arrow_dataset - Process #3 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-7b2331d0b396f378_00003_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #4 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-7b2331d0b396f378_00004_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-13 12:25:17 - INFO - datasets.arrow_dataset - Process #4 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-7b2331d0b396f378_00004_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #5 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-7b2331d0b396f378_00005_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-13 12:25:17 - INFO - datasets.arrow_dataset - Process #5 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-7b2331d0b396f378_00005_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #6 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-7b2331d0b396f378_00006_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-13 12:25:17 - INFO - datasets.arrow_dataset - Process #6 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-7b2331d0b396f378_00006_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #7 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-7b2331d0b396f378_00007_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-13 12:25:17 - INFO - datasets.arrow_dataset - Process #7 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-7b2331d0b396f378_00007_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #8 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-7b2331d0b396f378_00008_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-13 12:25:17 - INFO - datasets.arrow_dataset - Process #8 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-7b2331d0b396f378_00008_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #9 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-7b2331d0b396f378_00009_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-13 12:25:17 - INFO - datasets.arrow_dataset - Process #9 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-7b2331d0b396f378_00009_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-7b2331d0b396f378_*_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-13 12:25:21 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-7b2331d0b396f378_*_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concatenating 10 shards\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-13 12:25:21 - INFO - datasets.arrow_dataset - Concatenating 10 shards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-6b02c55b1008bc29.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-13 12:25:21 - INFO - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-6b02c55b1008bc29.arrow\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# Data Processing\n",
    "##################\n",
    "def apply_chat_template(\n",
    "    example,\n",
    "    tokenizer,\n",
    "):\n",
    "    messages = example[\"messages\"]['messages']\n",
    "    messages[0]['content'] = \"You are Pi-Card, the Raspberry Pi AI Assistant.\"\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False).strip('\\n')\n",
    "    return example\n",
    "\n",
    "raw_dataset = load_dataset(\"nkasmanoff/pi-card-sft-data\") \n",
    "\n",
    "\n",
    "train_dataset = raw_dataset[\"train\"]\n",
    "\n",
    "processed_train_dataset = train_dataset.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    num_proc=10,\n",
    "    desc=\"Applying chat template to train_sft\",\n",
    ")\n",
    "\n",
    "# shuffle the dataset\n",
    "processed_train_dataset = processed_train_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': {'messages': [{'content': 'You are Pi-Card, the Raspberry Pi AI Assistant.',\n",
       "    'role': 'system'},\n",
       "   {'content': 'How does Joel Salzberg\\'s book, \"Critical Essays on Bernard Malamud,\" contribute to the understanding of Malamud\\'s work in the context of American literature?: Essays and criticism on Bernard Malamud\\'s The Natural - Critical Essays.\\nEssays and criticism on Bernard Malamud - Malamud, Bernard.\\nAmazon.com: Critical Essays on Bernard Malamud (Critical Essays on American Literature) (9780816188819): Joel Salzberg: Books.\\nThe Natural by Bernard Malamud essaysIn his novel, \"The Natural\", Bernard Malamud presents a paradox of a character in Roy Hobbs. Roy is a talented athlete who fails to live up to the world\\'s expectations of him despite his good intentions. For all of his physical tools, Roy cannot overcome his.',\n",
       "    'role': 'user'},\n",
       "   {'content': 'Joel Salzberg\\'s book, \"Critical Essays on Bernard Malamud,\" provides in-depth analysis and insights into Malamud\\'s work, including \"The Natural,\" offering a deeper understanding of his themes, characters, and literary style within the context of American literature.',\n",
       "    'role': 'assistant'},\n",
       "   {'content': 'Can you recommend any specific essays from the book that are particularly insightful and worth reading?',\n",
       "    'role': 'user'},\n",
       "   {'content': 'Unfortunately, I don\\'t have specific information on the essays in Joel Salzberg\\'s book, \"Critical Essays on Bernard Malamud.\" However, I can suggest that you look for essays that focus on themes such as the American Dream, identity, and the human condition, as these are common threads in Malamud\\'s work, including \"The Natural.\"',\n",
       "    'role': 'assistant'},\n",
       "   {'content': 'Can you provide a brief summary of each essay so I can decide which one to read first?',\n",
       "    'role': 'user'},\n",
       "   {'content': 'I don\\'t have access to the specific essays in Joel Salzberg\\'s book, \"Critical Essays on Bernard Malamud.\" My training data doesn\\'t include detailed summaries of individual essays. I can suggest checking the book\\'s table of contents, introduction, or online reviews to get a sense of the topics and themes covered in the essays.',\n",
       "    'role': 'assistant'}]},\n",
       " 'text': '<|im_start|>system\\nYou are Pi-Card, the Raspberry Pi AI Assistant.<|im_end|>\\n<|im_start|>user\\nHow does Joel Salzberg\\'s book, \"Critical Essays on Bernard Malamud,\" contribute to the understanding of Malamud\\'s work in the context of American literature?: Essays and criticism on Bernard Malamud\\'s The Natural - Critical Essays.\\nEssays and criticism on Bernard Malamud - Malamud, Bernard.\\nAmazon.com: Critical Essays on Bernard Malamud (Critical Essays on American Literature) (9780816188819): Joel Salzberg: Books.\\nThe Natural by Bernard Malamud essaysIn his novel, \"The Natural\", Bernard Malamud presents a paradox of a character in Roy Hobbs. Roy is a talented athlete who fails to live up to the world\\'s expectations of him despite his good intentions. For all of his physical tools, Roy cannot overcome his.<|im_end|>\\n<|im_start|>assistant\\nJoel Salzberg\\'s book, \"Critical Essays on Bernard Malamud,\" provides in-depth analysis and insights into Malamud\\'s work, including \"The Natural,\" offering a deeper understanding of his themes, characters, and literary style within the context of American literature.<|im_end|>\\n<|im_start|>user\\nCan you recommend any specific essays from the book that are particularly insightful and worth reading?<|im_end|>\\n<|im_start|>assistant\\nUnfortunately, I don\\'t have specific information on the essays in Joel Salzberg\\'s book, \"Critical Essays on Bernard Malamud.\" However, I can suggest that you look for essays that focus on themes such as the American Dream, identity, and the human condition, as these are common threads in Malamud\\'s work, including \"The Natural.\"<|im_end|>\\n<|im_start|>user\\nCan you provide a brief summary of each essay so I can decide which one to read first?<|im_end|>\\n<|im_start|>assistant\\nI don\\'t have access to the specific essays in Joel Salzberg\\'s book, \"Critical Essays on Bernard Malamud.\" My training data doesn\\'t include detailed summaries of individual essays. I can suggest checking the book\\'s table of contents, introduction, or online reviews to get a sense of the topics and themes covered in the essays.<|im_end|>'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_train_dataset[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "\n",
      "I'm going on a wilderness<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "model.eval();\n",
    "system_prompt = \"You are Pi-Card, the Raspberry Pi AI Assistant.\"\n",
    "#prompt = \"\"\"What is the oort cloud?\"\"\"\n",
    "prompt = \"I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\"\n",
    "#prompt = f\"<|im_start|>system\\nYou are Pi-CARD, the Raspberry Pi AI assistant<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "prompt = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "input_ids = input_ids.to(model.device)\n",
    "output = model.generate(input_ids, max_new_tokens=256,  do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=False, pad_token_id = tokenizer.eos_token_id)\n",
    "formatted_output_text = \"<|im_end|>\".join(output_text.split(\"<|im_end|>\")[:2]) + \"<|im_end|>\"\n",
    "print(formatted_output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "[INFO|training_args.py:2100] 2024-11-13 12:25:39,675 >> PyTorch: setting up devices\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Loading cached processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-3d17a443d5786dd7.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-13 12:25:39 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-3d17a443d5786dd7.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2134] 2024-11-13 12:25:40,556 >> ***** Running training *****\n",
      "[INFO|trainer.py:2135] 2024-11-13 12:25:40,557 >>   Num examples = 5,660\n",
      "[INFO|trainer.py:2136] 2024-11-13 12:25:40,557 >>   Num Epochs = 8\n",
      "[INFO|trainer.py:2137] 2024-11-13 12:25:40,558 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:2140] 2024-11-13 12:25:40,559 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2141] 2024-11-13 12:25:40,559 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2142] 2024-11-13 12:25:40,560 >>   Total optimization steps = 5,664\n",
      "[INFO|trainer.py:2143] 2024-11-13 12:25:40,561 >>   Number of trainable parameters = 361,821,120\n",
      "[INFO|integration_utils.py:807] 2024-11-13 12:25:40,564 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5664' max='5664' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5664/5664 3:27:11, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.555100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.171400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.189600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.107400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.974000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.025400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.902000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.862200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.864700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.863500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.901600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.816100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.806500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.785100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.768300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.795400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.734000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.735200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.743100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.746300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.761400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.739800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.722400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.736200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.714900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.750900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.725900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.706800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.735400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.724300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.724500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.723100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.715400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>0.739100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3503] 2024-11-13 12:43:54,020 >> Saving model checkpoint to ./picard-ft/checkpoint-500\n",
      "[INFO|configuration_utils.py:472] 2024-11-13 12:43:54,022 >> Configuration saved in ./picard-ft/checkpoint-500/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-13 12:43:54,023 >> Configuration saved in ./picard-ft/checkpoint-500/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-13 12:43:55,323 >> Model weights saved in ./picard-ft/checkpoint-500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-13 12:43:55,325 >> tokenizer config file saved in ./picard-ft/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-13 12:43:55,326 >> Special tokens file saved in ./picard-ft/checkpoint-500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-13 12:43:58,064 >> tokenizer config file saved in ./picard-ft/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-13 12:43:58,065 >> Special tokens file saved in ./picard-ft/special_tokens_map.json\n",
      "[INFO|trainer.py:3503] 2024-11-13 13:02:07,629 >> Saving model checkpoint to ./picard-ft/checkpoint-1000\n",
      "[INFO|configuration_utils.py:472] 2024-11-13 13:02:07,631 >> Configuration saved in ./picard-ft/checkpoint-1000/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-13 13:02:07,632 >> Configuration saved in ./picard-ft/checkpoint-1000/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-13 13:02:08,867 >> Model weights saved in ./picard-ft/checkpoint-1000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-13 13:02:08,869 >> tokenizer config file saved in ./picard-ft/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-13 13:02:08,870 >> Special tokens file saved in ./picard-ft/checkpoint-1000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-13 13:02:12,130 >> tokenizer config file saved in ./picard-ft/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-13 13:02:12,131 >> Special tokens file saved in ./picard-ft/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-13 13:02:12,184 >> Deleting older checkpoint [picard-ft/checkpoint-500] due to args.save_total_limit\n",
      "[INFO|trainer.py:3503] 2024-11-13 13:20:19,182 >> Saving model checkpoint to ./picard-ft/checkpoint-1500\n",
      "[INFO|configuration_utils.py:472] 2024-11-13 13:20:19,184 >> Configuration saved in ./picard-ft/checkpoint-1500/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-13 13:20:19,185 >> Configuration saved in ./picard-ft/checkpoint-1500/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-13 13:20:20,380 >> Model weights saved in ./picard-ft/checkpoint-1500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-13 13:20:20,382 >> tokenizer config file saved in ./picard-ft/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-13 13:20:20,383 >> Special tokens file saved in ./picard-ft/checkpoint-1500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-13 13:20:23,701 >> tokenizer config file saved in ./picard-ft/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-13 13:20:23,702 >> Special tokens file saved in ./picard-ft/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-13 13:20:23,737 >> Deleting older checkpoint [picard-ft/checkpoint-1000] due to args.save_total_limit\n",
      "[INFO|trainer.py:3503] 2024-11-13 13:38:39,955 >> Saving model checkpoint to ./picard-ft/checkpoint-2000\n",
      "[INFO|configuration_utils.py:472] 2024-11-13 13:38:39,956 >> Configuration saved in ./picard-ft/checkpoint-2000/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-13 13:38:39,957 >> Configuration saved in ./picard-ft/checkpoint-2000/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-13 13:38:41,219 >> Model weights saved in ./picard-ft/checkpoint-2000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-13 13:38:41,221 >> tokenizer config file saved in ./picard-ft/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-13 13:38:41,222 >> Special tokens file saved in ./picard-ft/checkpoint-2000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-13 13:38:44,541 >> tokenizer config file saved in ./picard-ft/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-13 13:38:44,542 >> Special tokens file saved in ./picard-ft/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-13 13:38:44,575 >> Deleting older checkpoint [picard-ft/checkpoint-1500] due to args.save_total_limit\n",
      "[INFO|trainer.py:3503] 2024-11-13 13:56:20,326 >> Saving model checkpoint to ./picard-ft/checkpoint-2500\n",
      "[INFO|configuration_utils.py:472] 2024-11-13 13:56:20,328 >> Configuration saved in ./picard-ft/checkpoint-2500/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-13 13:56:20,329 >> Configuration saved in ./picard-ft/checkpoint-2500/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-13 13:56:21,605 >> Model weights saved in ./picard-ft/checkpoint-2500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-13 13:56:21,607 >> tokenizer config file saved in ./picard-ft/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-13 13:56:21,608 >> Special tokens file saved in ./picard-ft/checkpoint-2500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-13 13:56:24,922 >> tokenizer config file saved in ./picard-ft/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-13 13:56:24,923 >> Special tokens file saved in ./picard-ft/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-13 13:56:24,957 >> Deleting older checkpoint [picard-ft/checkpoint-2000] due to args.save_total_limit\n",
      "[INFO|trainer.py:3503] 2024-11-13 14:15:27,116 >> Saving model checkpoint to ./picard-ft/checkpoint-3000\n",
      "[INFO|configuration_utils.py:472] 2024-11-13 14:15:27,119 >> Configuration saved in ./picard-ft/checkpoint-3000/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-13 14:15:27,120 >> Configuration saved in ./picard-ft/checkpoint-3000/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-13 14:15:28,330 >> Model weights saved in ./picard-ft/checkpoint-3000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-13 14:15:28,332 >> tokenizer config file saved in ./picard-ft/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-13 14:15:28,333 >> Special tokens file saved in ./picard-ft/checkpoint-3000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-13 14:15:31,583 >> tokenizer config file saved in ./picard-ft/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-13 14:15:31,584 >> Special tokens file saved in ./picard-ft/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-13 14:15:31,618 >> Deleting older checkpoint [picard-ft/checkpoint-2500] due to args.save_total_limit\n",
      "[INFO|trainer.py:3503] 2024-11-13 14:33:43,738 >> Saving model checkpoint to ./picard-ft/checkpoint-3500\n",
      "[INFO|configuration_utils.py:472] 2024-11-13 14:33:43,740 >> Configuration saved in ./picard-ft/checkpoint-3500/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-13 14:33:43,741 >> Configuration saved in ./picard-ft/checkpoint-3500/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-13 14:33:44,990 >> Model weights saved in ./picard-ft/checkpoint-3500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-13 14:33:44,992 >> tokenizer config file saved in ./picard-ft/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-13 14:33:44,993 >> Special tokens file saved in ./picard-ft/checkpoint-3500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-13 14:33:48,098 >> tokenizer config file saved in ./picard-ft/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-13 14:33:48,099 >> Special tokens file saved in ./picard-ft/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-13 14:33:48,135 >> Deleting older checkpoint [picard-ft/checkpoint-3000] due to args.save_total_limit\n",
      "[INFO|trainer.py:3503] 2024-11-13 14:52:15,671 >> Saving model checkpoint to ./picard-ft/checkpoint-4000\n",
      "[INFO|configuration_utils.py:472] 2024-11-13 14:52:15,673 >> Configuration saved in ./picard-ft/checkpoint-4000/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-13 14:52:15,673 >> Configuration saved in ./picard-ft/checkpoint-4000/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-13 14:52:16,993 >> Model weights saved in ./picard-ft/checkpoint-4000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-13 14:52:16,995 >> tokenizer config file saved in ./picard-ft/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-13 14:52:16,996 >> Special tokens file saved in ./picard-ft/checkpoint-4000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-13 14:52:20,565 >> tokenizer config file saved in ./picard-ft/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-13 14:52:20,566 >> Special tokens file saved in ./picard-ft/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-13 14:52:20,601 >> Deleting older checkpoint [picard-ft/checkpoint-3500] due to args.save_total_limit\n",
      "[INFO|trainer.py:3503] 2024-11-13 15:10:28,080 >> Saving model checkpoint to ./picard-ft/checkpoint-4500\n",
      "[INFO|configuration_utils.py:472] 2024-11-13 15:10:28,082 >> Configuration saved in ./picard-ft/checkpoint-4500/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-13 15:10:28,083 >> Configuration saved in ./picard-ft/checkpoint-4500/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-13 15:10:29,348 >> Model weights saved in ./picard-ft/checkpoint-4500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-13 15:10:29,350 >> tokenizer config file saved in ./picard-ft/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-13 15:10:29,351 >> Special tokens file saved in ./picard-ft/checkpoint-4500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-13 15:10:32,606 >> tokenizer config file saved in ./picard-ft/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-13 15:10:32,607 >> Special tokens file saved in ./picard-ft/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-13 15:10:32,641 >> Deleting older checkpoint [picard-ft/checkpoint-4000] due to args.save_total_limit\n",
      "[INFO|trainer.py:3503] 2024-11-13 15:28:29,516 >> Saving model checkpoint to ./picard-ft/checkpoint-5000\n",
      "[INFO|configuration_utils.py:472] 2024-11-13 15:28:29,518 >> Configuration saved in ./picard-ft/checkpoint-5000/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-13 15:28:29,519 >> Configuration saved in ./picard-ft/checkpoint-5000/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-13 15:28:30,777 >> Model weights saved in ./picard-ft/checkpoint-5000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-13 15:28:30,780 >> tokenizer config file saved in ./picard-ft/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-13 15:28:30,781 >> Special tokens file saved in ./picard-ft/checkpoint-5000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-13 15:28:34,079 >> tokenizer config file saved in ./picard-ft/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-13 15:28:34,080 >> Special tokens file saved in ./picard-ft/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-13 15:28:34,113 >> Deleting older checkpoint [picard-ft/checkpoint-4500] due to args.save_total_limit\n",
      "[INFO|trainer.py:3503] 2024-11-13 15:46:21,586 >> Saving model checkpoint to ./picard-ft/checkpoint-5500\n",
      "[INFO|configuration_utils.py:472] 2024-11-13 15:46:21,588 >> Configuration saved in ./picard-ft/checkpoint-5500/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-13 15:46:21,589 >> Configuration saved in ./picard-ft/checkpoint-5500/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-13 15:46:22,884 >> Model weights saved in ./picard-ft/checkpoint-5500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-13 15:46:22,886 >> tokenizer config file saved in ./picard-ft/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-13 15:46:22,886 >> Special tokens file saved in ./picard-ft/checkpoint-5500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-13 15:46:26,161 >> tokenizer config file saved in ./picard-ft/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-13 15:46:26,162 >> Special tokens file saved in ./picard-ft/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-13 15:46:26,196 >> Deleting older checkpoint [picard-ft/checkpoint-5000] due to args.save_total_limit\n",
      "[INFO|trainer.py:3503] 2024-11-13 15:52:50,167 >> Saving model checkpoint to ./picard-ft/checkpoint-5664\n",
      "[INFO|configuration_utils.py:472] 2024-11-13 15:52:50,169 >> Configuration saved in ./picard-ft/checkpoint-5664/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-13 15:52:50,170 >> Configuration saved in ./picard-ft/checkpoint-5664/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-13 15:52:51,433 >> Model weights saved in ./picard-ft/checkpoint-5664/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-13 15:52:51,435 >> tokenizer config file saved in ./picard-ft/checkpoint-5664/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-13 15:52:51,436 >> Special tokens file saved in ./picard-ft/checkpoint-5664/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-13 15:52:54,790 >> tokenizer config file saved in ./picard-ft/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-13 15:52:54,791 >> Special tokens file saved in ./picard-ft/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-13 15:52:54,826 >> Deleting older checkpoint [picard-ft/checkpoint-5500] due to args.save_total_limit\n",
      "[INFO|trainer.py:2394] 2024-11-13 15:52:54,828 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:4283] 2024-11-13 15:52:54,830 >> Waiting for the current checkpoint push to be finished, this might take a couple of minutes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =         8.0\n",
      "  total_flos               = 121992911GF\n",
      "  train_loss               =      0.8536\n",
      "  train_runtime            =  3:27:14.26\n",
      "  train_samples_per_second =       3.642\n",
      "  train_steps_per_second   =       0.456\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# Training\n",
    "###########\n",
    "\n",
    "model.train();\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=train_conf,\n",
    "    train_dataset=processed_train_dataset,\n",
    "    max_seq_length=2048,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    #packing=True,\n",
    ")\n",
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "# trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the checkpoint\n",
    "\n",
    "# find most recently created folder in checkpoint_dir and set as checkpoint path\n",
    "checkpoint_path = sorted(os.listdir(train_conf.output_dir))[-1]\n",
    "checkpoint_path = os.path.join(train_conf.output_dir, checkpoint_path)\n",
    "model_kwargs = dict(\n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    "     torch_dtype=torch.bfloat16,\n",
    "   device_map='auto'\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_path, **model_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Pi-Card, the Raspberry Pi AI Assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Who is George RR Martin?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "George R.R. Martin is a renowned American author, best known for his epic fantasy series \"A Song of Ice and Fire,\" which has become a cultural phenomenon.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "model.eval();\n",
    "prompt = \"\"\"Who is George RR Martin?\"\"\"\n",
    "prompt = f\"<|im_start|>system\\nYou are Pi-Card, the Raspberry Pi AI Assistant.<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "input_ids = input_ids.to(model.device)\n",
    "output = model.generate(input_ids, max_new_tokens=256,  do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=False, pad_token_id = tokenizer.eos_token_id)\n",
    "formatted_output_text = \"<|im_end|>\".join(output_text.split(\"<|im_end|>\")[:3]) + \"<|im_end|>\" \n",
    "print(formatted_output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving to gguf\n",
    "#https://github.com/ggerganov/llama.cpp/discussions/2948\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by downloading llama-cpp if not already done\n",
    "\n",
    "#!git clone https://github.com/ggerganov/llama.cpp.git\n",
    "!pip install -r llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gguf file\n",
    "\n",
    "# Please note you'll need to update the checkpoint path and model names to the one you want to convert & save\n",
    "!python llama.cpp/convert_hf_to_gguf.py nature-buddy/checkpoint-2005 --outfile nature-buddy-0.135b-f16.gguf --outtype f16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quanitzation output is going to have an outsized impact on latency / performance. \n",
    "\n",
    "While f16 is the default and good, it's worth noting the model was trained using bf16, a slightly different format, so that outtype may be worth testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the gguf you can either work with that directly, or convert it to an ollama format, which can be easier to work with in some cases. \n",
    "\n",
    "For instructions on how to do this, please see the instructions in create ollama text file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

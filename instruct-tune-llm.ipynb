{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruct tuning the model\n",
    "\n",
    "This notebook draws heavily a similar one done for the [phi3](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/sample_finetune.py) model. \n",
    "\n",
    "The difference here is that this will focus on a model's full fine-tuning process, work for going from a base model to a new insruction model, and should work for almost any model on HuggingFace.\n",
    "\n",
    "At the end of the notebook are the steps to save this as a gguf format which will allow for fast and easy inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "import os\n",
    "import json\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnoahpunintended\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20241115_184833-joca0fed</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/noahpunintended/smollm-ft/runs/joca0fed' target=\"_blank\">quiet-planet-42</a></strong> to <a href='https://wandb.ai/noahpunintended/smollm-ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/noahpunintended/smollm-ft' target=\"_blank\">https://wandb.ai/noahpunintended/smollm-ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/noahpunintended/smollm-ft/runs/joca0fed' target=\"_blank\">https://wandb.ai/noahpunintended/smollm-ft/runs/joca0fed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "wandb.init(project=\"smollm-ft\")\n",
    "###################\n",
    "# Hyper-parameters\n",
    "###################\n",
    "training_config = {\n",
    "    \"do_eval\": False,\n",
    "    \"learning_rate\": 5.0e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"per_device_train_batch_size\": 8,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"log_level\": \"info\",\n",
    "    \"logging_steps\": 150,\n",
    "    \"logging_strategy\": \"steps\",\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"num_train_epochs\": 5,\n",
    "    \"max_steps\": -1,\n",
    "    \"output_dir\": \"./picard-smol-ft\",\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"remove_unused_columns\": True,\n",
    "    \"save_steps\": 500,\n",
    "    \"save_total_limit\": 1,\n",
    "    \"seed\": 0,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"gradient_checkpointing_kwargs\":{\"use_reentrant\": False},\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"warmup_ratio\": 0.05,\n",
    "    \"report_to\":\"wandb\",\n",
    "    \"neftune_noise_alpha\":5,\n",
    "    \"push_to_hub\": True,\n",
    "    }\n",
    "\n",
    "train_conf = TrainingArguments(**training_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 18:48:36 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=no,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs={'use_reentrant': False},\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0005,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./picard-smol-ft/runs/Nov15_18-48-36_ip-10-192-12-216,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=150,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=5,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./picard-smol-ft,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=True,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./picard-smol-ft,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=0,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# Setup logging\n",
    "###############\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "log_level = train_conf.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "logger.info(f\"Training/evaluation parameters {train_conf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-11-15 18:48:36,532 >> loading configuration file config.json from cache at /home/zeus/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M/snapshots/3ce05f63c246c44616da500b47b01f082f4d3bcc/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-11-15 18:48:36,534 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"HuggingFaceTB/SmolLM2-360M\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 960,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2560,\n",
      "  \"is_llama_config\": true,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 15,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 5,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_interleaved\": false,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 49152\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3678] 2024-11-15 18:48:36,608 >> loading weights file model.safetensors from cache at /home/zeus/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M/snapshots/3ce05f63c246c44616da500b47b01f082f4d3bcc/model.safetensors\n",
      "[INFO|modeling_utils.py:1606] 2024-11-15 18:48:36,708 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1038] 2024-11-15 18:48:36,711 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4507] 2024-11-15 18:48:37,299 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4515] 2024-11-15 18:48:37,300 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at HuggingFaceTB/SmolLM2-360M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:993] 2024-11-15 18:48:37,361 >> loading configuration file generation_config.json from cache at /home/zeus/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M/snapshots/3ce05f63c246c44616da500b47b01f082f4d3bcc/generation_config.json\n",
      "[INFO|configuration_utils.py:1038] 2024-11-15 18:48:37,361 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-11-15 18:48:37,512 >> loading file vocab.json from cache at /home/zeus/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M-Instruct/snapshots/4873f67095301d304753fae05bc09ec766634e50/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-11-15 18:48:37,513 >> loading file merges.txt from cache at /home/zeus/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M-Instruct/snapshots/4873f67095301d304753fae05bc09ec766634e50/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-11-15 18:48:37,514 >> loading file tokenizer.json from cache at /home/zeus/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M-Instruct/snapshots/4873f67095301d304753fae05bc09ec766634e50/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-11-15 18:48:37,514 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-11-15 18:48:37,514 >> loading file special_tokens_map.json from cache at /home/zeus/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M-Instruct/snapshots/4873f67095301d304753fae05bc09ec766634e50/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-11-15 18:48:37,515 >> loading file tokenizer_config.json from cache at /home/zeus/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M-Instruct/snapshots/4873f67095301d304753fae05bc09ec766634e50/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "####################\n",
    "# Base Model Loading\n",
    "####################\n",
    "checkpoint_path = \"HuggingFaceTB/SmolLM2-360M\"\n",
    "model_kwargs = dict(\n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    "#    attn_implementation=\"flash_attention_2\",  # only works on latest gpus, probably not worth it in most cases\n",
    "     torch_dtype=torch.bfloat16,\n",
    "   device_map='auto'\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_path, **model_kwargs)\n",
    "\n",
    "\n",
    "###################\n",
    "# Tokenizer Loading\n",
    "###################\n",
    "\n",
    "checkpoint_path = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "tokenizer.model_max_length = 2048\n",
    "tokenizer.pad_token = \"<|endoftext|>\"  # note this is specific to smollm\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "tokenizer.padding_side = 'right'\n",
    "# https://stackoverflow.com/questions/76446228/setting-padding-token-as-eos-token-when-using-datacollatorforlanguagemodeling-fr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|endoftext|>', '<|im_end|>', 2, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token, tokenizer.eos_token, tokenizer.eos_token_id, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the fine-tune \n",
    "\n",
    "Now that the synthetic dataset is made, next up is ensure the model is capable of answering like we expect, without the large system prompt impacting latency. \n",
    "\n",
    "The solution to this is to open up the dataset, replace the system prompt with something much simpler, and starting training with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 18:48:38 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 18:48:38 - INFO - datasets.info - Loading Dataset info from /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset pi-card-sft-data (/home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 18:48:38 - INFO - datasets.builder - Found cached dataset pi-card-sft-data (/home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 18:48:38 - INFO - datasets.info - Loading Dataset info from /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #0 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-22813cdd6ab427cc_00000_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 18:48:39 - INFO - datasets.arrow_dataset - Process #0 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-22813cdd6ab427cc_00000_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #1 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-22813cdd6ab427cc_00001_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 18:48:39 - INFO - datasets.arrow_dataset - Process #1 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-22813cdd6ab427cc_00001_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #2 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-22813cdd6ab427cc_00002_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 18:48:39 - INFO - datasets.arrow_dataset - Process #2 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-22813cdd6ab427cc_00002_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #3 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-22813cdd6ab427cc_00003_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 18:48:39 - INFO - datasets.arrow_dataset - Process #3 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-22813cdd6ab427cc_00003_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #4 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-22813cdd6ab427cc_00004_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 18:48:39 - INFO - datasets.arrow_dataset - Process #4 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-22813cdd6ab427cc_00004_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #5 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-22813cdd6ab427cc_00005_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 18:48:39 - INFO - datasets.arrow_dataset - Process #5 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-22813cdd6ab427cc_00005_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #6 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-22813cdd6ab427cc_00006_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 18:48:39 - INFO - datasets.arrow_dataset - Process #6 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-22813cdd6ab427cc_00006_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #7 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-22813cdd6ab427cc_00007_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 18:48:39 - INFO - datasets.arrow_dataset - Process #7 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-22813cdd6ab427cc_00007_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #8 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-22813cdd6ab427cc_00008_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 18:48:39 - INFO - datasets.arrow_dataset - Process #8 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-22813cdd6ab427cc_00008_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #9 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-22813cdd6ab427cc_00009_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 18:48:39 - INFO - datasets.arrow_dataset - Process #9 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-22813cdd6ab427cc_00009_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-22813cdd6ab427cc_*_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 18:48:41 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-22813cdd6ab427cc_*_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concatenating 10 shards\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 18:48:41 - INFO - datasets.arrow_dataset - Concatenating 10 shards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-68cc631114fc8bcc.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 18:48:41 - INFO - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-68cc631114fc8bcc.arrow\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# Data Processing\n",
    "##################\n",
    "def apply_chat_template(\n",
    "    example,\n",
    "    tokenizer,\n",
    "):\n",
    "    messages = example[\"messages\"]['messages']\n",
    "    messages[0]['content'] = \"You are Pi-Card.\" # Or just remove?\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False).strip('\\n')\n",
    "    return example\n",
    "\n",
    "raw_dataset = load_dataset(\"nkasmanoff/pi-card-sft-data\") \n",
    "\n",
    "\n",
    "train_dataset = raw_dataset[\"train\"]\n",
    "\n",
    "processed_train_dataset = train_dataset.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    num_proc=10,\n",
    "    desc=\"Applying chat template to train_sft\",\n",
    ")\n",
    "\n",
    "# shuffle the dataset\n",
    "processed_train_dataset = processed_train_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': {'messages': [{'content': 'You are Pi-Card.', 'role': 'system'},\n",
       "   {'content': 'How are clouds formed?', 'role': 'user'},\n",
       "   {'content': 'Clouds are formed when water vapor in the air condenses into visible liquid droplets or ice crystals, typically as a result of changes in temperature or humidity.',\n",
       "    'role': 'assistant'}]},\n",
       " 'text': '<|im_start|>system\\nYou are Pi-Card.<|im_end|>\\n<|im_start|>user\\nHow are clouds formed?<|im_end|>\\n<|im_start|>assistant\\nClouds are formed when water vapor in the air condenses into visible liquid droplets or ice crystals, typically as a result of changes in temperature or humidity.<|im_end|>'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_train_dataset[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "\n",
      "I'm going on a wilderness<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "model.eval();\n",
    "system_prompt = \"You are Pi-Card.\"\n",
    "#prompt = \"\"\"What is the oort cloud?\"\"\"\n",
    "prompt = \"I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\"\n",
    "#prompt = f\"<|im_start|>system\\nYou are Pi-CARD, the Raspberry Pi AI assistant<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "prompt = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "input_ids = input_ids.to(model.device)\n",
    "output = model.generate(input_ids, max_new_tokens=256,  do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=False, pad_token_id = tokenizer.eos_token_id)\n",
    "formatted_output_text = \"<|im_end|>\".join(output_text.split(\"<|im_end|>\")[:2]) + \"<|im_end|>\"\n",
    "print(formatted_output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "[INFO|training_args.py:2100] 2024-11-15 18:48:42,773 >> PyTorch: setting up devices\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Loading cached processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-0c9a379ef98ce0cf.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 18:48:43 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___pi-card-sft-data/default/0.0.0/ace7c218bc809b351ad35add013fe25169b7c2d6/cache-0c9a379ef98ce0cf.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2134] 2024-11-15 18:48:44,687 >> ***** Running training *****\n",
      "[INFO|trainer.py:2135] 2024-11-15 18:48:44,688 >>   Num examples = 5,660\n",
      "[INFO|trainer.py:2136] 2024-11-15 18:48:44,689 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:2137] 2024-11-15 18:48:44,689 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:2140] 2024-11-15 18:48:44,690 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2141] 2024-11-15 18:48:44,690 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2142] 2024-11-15 18:48:44,691 >>   Total optimization steps = 3,540\n",
      "[INFO|trainer.py:2143] 2024-11-15 18:48:44,692 >>   Number of trainable parameters = 361,821,120\n",
      "[INFO|integration_utils.py:807] 2024-11-15 18:48:44,693 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3540' max='3540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3540/3540 2:09:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.386000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.258300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.198900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.110600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.843500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.838100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.811500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.857700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.634800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.519200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.507400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.507200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.526700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.399600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.371800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.360100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.346700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.364600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.331200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.326000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.330700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.323500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3503] 2024-11-15 19:07:03,094 >> Saving model checkpoint to ./picard-smol-ft/checkpoint-500\n",
      "[INFO|configuration_utils.py:472] 2024-11-15 19:07:03,096 >> Configuration saved in ./picard-smol-ft/checkpoint-500/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-15 19:07:03,097 >> Configuration saved in ./picard-smol-ft/checkpoint-500/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-15 19:07:04,431 >> Model weights saved in ./picard-smol-ft/checkpoint-500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-15 19:07:04,433 >> tokenizer config file saved in ./picard-smol-ft/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-15 19:07:04,434 >> Special tokens file saved in ./picard-smol-ft/checkpoint-500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-15 19:07:07,822 >> tokenizer config file saved in ./picard-smol-ft/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-15 19:07:07,823 >> Special tokens file saved in ./picard-smol-ft/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-15 19:07:07,856 >> Deleting older checkpoint [picard-smol-ft/checkpoint-500] due to args.save_total_limit\n",
      "[INFO|trainer.py:3503] 2024-11-15 19:25:18,679 >> Saving model checkpoint to ./picard-smol-ft/checkpoint-1000\n",
      "[INFO|configuration_utils.py:472] 2024-11-15 19:25:18,681 >> Configuration saved in ./picard-smol-ft/checkpoint-1000/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-15 19:25:18,682 >> Configuration saved in ./picard-smol-ft/checkpoint-1000/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-15 19:25:20,031 >> Model weights saved in ./picard-smol-ft/checkpoint-1000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-15 19:25:20,033 >> tokenizer config file saved in ./picard-smol-ft/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-15 19:25:20,034 >> Special tokens file saved in ./picard-smol-ft/checkpoint-1000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-15 19:25:22,815 >> tokenizer config file saved in ./picard-smol-ft/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-15 19:25:22,816 >> Special tokens file saved in ./picard-smol-ft/special_tokens_map.json\n",
      "[INFO|trainer.py:3503] 2024-11-15 19:43:26,995 >> Saving model checkpoint to ./picard-smol-ft/checkpoint-1500\n",
      "[INFO|configuration_utils.py:472] 2024-11-15 19:43:26,997 >> Configuration saved in ./picard-smol-ft/checkpoint-1500/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-15 19:43:26,998 >> Configuration saved in ./picard-smol-ft/checkpoint-1500/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-15 19:43:28,167 >> Model weights saved in ./picard-smol-ft/checkpoint-1500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-15 19:43:28,169 >> tokenizer config file saved in ./picard-smol-ft/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-15 19:43:28,170 >> Special tokens file saved in ./picard-smol-ft/checkpoint-1500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-15 19:43:31,528 >> tokenizer config file saved in ./picard-smol-ft/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-15 19:43:31,529 >> Special tokens file saved in ./picard-smol-ft/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-15 19:43:31,565 >> Deleting older checkpoint [picard-smol-ft/checkpoint-1000] due to args.save_total_limit\n",
      "[INFO|trainer.py:3503] 2024-11-15 20:01:46,256 >> Saving model checkpoint to ./picard-smol-ft/checkpoint-2000\n",
      "[INFO|configuration_utils.py:472] 2024-11-15 20:01:46,257 >> Configuration saved in ./picard-smol-ft/checkpoint-2000/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-15 20:01:46,258 >> Configuration saved in ./picard-smol-ft/checkpoint-2000/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-15 20:01:47,498 >> Model weights saved in ./picard-smol-ft/checkpoint-2000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-15 20:01:47,500 >> tokenizer config file saved in ./picard-smol-ft/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-15 20:01:47,501 >> Special tokens file saved in ./picard-smol-ft/checkpoint-2000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-15 20:01:50,903 >> tokenizer config file saved in ./picard-smol-ft/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-15 20:01:50,905 >> Special tokens file saved in ./picard-smol-ft/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-15 20:01:50,940 >> Deleting older checkpoint [picard-smol-ft/checkpoint-1500] due to args.save_total_limit\n",
      "[INFO|trainer.py:3503] 2024-11-15 20:19:18,285 >> Saving model checkpoint to ./picard-smol-ft/checkpoint-2500\n",
      "[INFO|configuration_utils.py:472] 2024-11-15 20:19:18,287 >> Configuration saved in ./picard-smol-ft/checkpoint-2500/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-15 20:19:18,288 >> Configuration saved in ./picard-smol-ft/checkpoint-2500/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-15 20:19:19,673 >> Model weights saved in ./picard-smol-ft/checkpoint-2500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-15 20:19:19,676 >> tokenizer config file saved in ./picard-smol-ft/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-15 20:19:19,677 >> Special tokens file saved in ./picard-smol-ft/checkpoint-2500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-15 20:19:22,958 >> tokenizer config file saved in ./picard-smol-ft/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-15 20:19:22,959 >> Special tokens file saved in ./picard-smol-ft/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-15 20:19:22,997 >> Deleting older checkpoint [picard-smol-ft/checkpoint-2000] due to args.save_total_limit\n",
      "[INFO|trainer.py:3503] 2024-11-15 20:38:17,997 >> Saving model checkpoint to ./picard-smol-ft/checkpoint-3000\n",
      "[INFO|configuration_utils.py:472] 2024-11-15 20:38:17,999 >> Configuration saved in ./picard-smol-ft/checkpoint-3000/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-15 20:38:18,000 >> Configuration saved in ./picard-smol-ft/checkpoint-3000/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-15 20:38:19,257 >> Model weights saved in ./picard-smol-ft/checkpoint-3000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-15 20:38:19,259 >> tokenizer config file saved in ./picard-smol-ft/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-15 20:38:19,260 >> Special tokens file saved in ./picard-smol-ft/checkpoint-3000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-15 20:38:22,603 >> tokenizer config file saved in ./picard-smol-ft/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-15 20:38:22,604 >> Special tokens file saved in ./picard-smol-ft/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-15 20:38:22,639 >> Deleting older checkpoint [picard-smol-ft/checkpoint-2500] due to args.save_total_limit\n",
      "[INFO|trainer.py:3503] 2024-11-15 20:56:30,240 >> Saving model checkpoint to ./picard-smol-ft/checkpoint-3500\n",
      "[INFO|configuration_utils.py:472] 2024-11-15 20:56:30,241 >> Configuration saved in ./picard-smol-ft/checkpoint-3500/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-15 20:56:30,242 >> Configuration saved in ./picard-smol-ft/checkpoint-3500/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-15 20:56:31,527 >> Model weights saved in ./picard-smol-ft/checkpoint-3500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-15 20:56:31,530 >> tokenizer config file saved in ./picard-smol-ft/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-15 20:56:31,530 >> Special tokens file saved in ./picard-smol-ft/checkpoint-3500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-15 20:56:34,782 >> tokenizer config file saved in ./picard-smol-ft/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-15 20:56:34,783 >> Special tokens file saved in ./picard-smol-ft/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-15 20:56:34,818 >> Deleting older checkpoint [picard-smol-ft/checkpoint-3000] due to args.save_total_limit\n",
      "[INFO|trainer.py:3503] 2024-11-15 20:57:56,159 >> Saving model checkpoint to ./picard-smol-ft/checkpoint-3540\n",
      "[INFO|configuration_utils.py:472] 2024-11-15 20:57:56,160 >> Configuration saved in ./picard-smol-ft/checkpoint-3540/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-15 20:57:56,161 >> Configuration saved in ./picard-smol-ft/checkpoint-3540/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-15 20:57:57,384 >> Model weights saved in ./picard-smol-ft/checkpoint-3540/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-15 20:57:57,386 >> tokenizer config file saved in ./picard-smol-ft/checkpoint-3540/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-15 20:57:57,387 >> Special tokens file saved in ./picard-smol-ft/checkpoint-3540/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-15 20:58:00,742 >> tokenizer config file saved in ./picard-smol-ft/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-15 20:58:00,744 >> Special tokens file saved in ./picard-smol-ft/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-15 20:58:00,779 >> Deleting older checkpoint [picard-smol-ft/checkpoint-3500] due to args.save_total_limit\n",
      "[INFO|trainer.py:2394] 2024-11-15 20:58:01,035 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:4283] 2024-11-15 20:58:01,038 >> Waiting for the current checkpoint push to be finished, this might take a couple of minutes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  total_flos               = 76087062GF\n",
      "  train_loss               =     0.6602\n",
      "  train_runtime            = 2:09:16.34\n",
      "  train_samples_per_second =      3.649\n",
      "  train_steps_per_second   =      0.456\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# Training\n",
    "###########\n",
    "\n",
    "model.train();\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=train_conf,\n",
    "    train_dataset=processed_train_dataset,\n",
    "    max_seq_length=2048,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    #packing=True,\n",
    ")\n",
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "# trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the checkpoint\n",
    "\n",
    "# find most recently created folder in checkpoint_dir and set as checkpoint path\n",
    "checkpoint_path = sorted(os.listdir(train_conf.output_dir))[-1]\n",
    "checkpoint_path = os.path.join(train_conf.output_dir, checkpoint_path)\n",
    "model_kwargs = dict(\n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    "     torch_dtype=torch.bfloat16,\n",
    "   device_map='auto'\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_path, **model_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Pi-Card.<|im_end|>\n",
      "<|im_start|>user\n",
      "I have 45 pills. Sofie dose is 1 pill in morning and half pill at night. How long will this last<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To find the number of days the medication will last, we need to divide the total number of pills by the dose of each pill. Since there are 32 pills in a box for Sofie, and each box contains 4 pills, the medication will last for 12 days if taken as directed.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "model.eval();\n",
    "#prompt = \"\"\"Who are you?\"\"\"\n",
    "prompt = \"\"\"I have 45 pills. Sofie dose is 1 pill in morning and half pill at night. How long will this last\"\"\"\n",
    "prompt = f\"<|im_start|>system\\nYou are Pi-Card.<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "input_ids = input_ids.to(model.device)\n",
    "output = model.generate(input_ids, max_new_tokens=256,  do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=False, pad_token_id = tokenizer.eos_token_id)\n",
    "formatted_output_text = \"<|im_end|>\".join(output_text.split(\"<|im_end|>\")[:3]) + \"<|im_end|>\" \n",
    "print(formatted_output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving to gguf\n",
    "#https://github.com/ggerganov/llama.cpp/discussions/2948\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by downloading llama-cpp if not already done\n",
    "\n",
    "#!git clone https://github.com/ggerganov/llama.cpp.git\n",
    "!pip install -r llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gguf file\n",
    "\n",
    "# Please note you'll need to update the checkpoint path and model names to the one you want to convert & save\n",
    "!python llama.cpp/convert_hf_to_gguf.py nature-buddy/checkpoint-2005 --outfile nature-buddy-0.135b-f16.gguf --outtype f16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quanitzation output is going to have an outsized impact on latency / performance. \n",
    "\n",
    "While f16 is the default and good, it's worth noting the model was trained using bf16, a slightly different format, so that outtype may be worth testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the gguf you can either work with that directly, or convert it to an ollama format, which can be easier to work with in some cases. \n",
    "\n",
    "For instructions on how to do this, please see the instructions in create ollama text file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruct tuning the model\n",
    "\n",
    "This notebook draws heavily a similar one done for the [phi3](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/sample_finetune.py) model. \n",
    "\n",
    "The difference here is that this will focus on a model's full fine-tuning process, work for going from a base model to a new insruction model, and should work for almost any model on HuggingFace.\n",
    "\n",
    "At the end of the notebook are the steps to save this as a gguf format which will allow for fast and easy inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "import os\n",
    "import json\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnoahpunintended\u001b[0m (\u001b[33mfdlx\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20241107_224018-lug4bbuw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fdlx/smollm-ft/runs/lug4bbuw' target=\"_blank\">curious-feather-4</a></strong> to <a href='https://wandb.ai/fdlx/smollm-ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fdlx/smollm-ft' target=\"_blank\">https://wandb.ai/fdlx/smollm-ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fdlx/smollm-ft/runs/lug4bbuw' target=\"_blank\">https://wandb.ai/fdlx/smollm-ft/runs/lug4bbuw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "wandb.init(project=\"smollm-ft\")\n",
    "###################\n",
    "# Hyper-parameters\n",
    "###################\n",
    "training_config = {\n",
    "    \"do_eval\": False,\n",
    "    \"learning_rate\": 5.0e-04,\n",
    "    \"per_device_train_batch_size\": 8,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"log_level\": \"info\",\n",
    "    \"logging_steps\": 100,\n",
    "    \"logging_strategy\": \"steps\",\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"num_train_epochs\": 8,\n",
    "    \"max_steps\": -1,\n",
    "    \"output_dir\": \"./nature-buddy\",\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"remove_unused_columns\": True,\n",
    "    \"save_steps\": 500,\n",
    "    \"save_total_limit\": 1,\n",
    "    \"seed\": 0,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"gradient_checkpointing_kwargs\":{\"use_reentrant\": False},\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"warmup_ratio\": 0.05,\n",
    "    \"report_to\":\"wandb\",\n",
    "    \"neftune_noise_alpha\":3,\n",
    "    \"push_to_hub\": True,\n",
    "    }\n",
    "\n",
    "train_conf = TrainingArguments(**training_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:40:21 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=no,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs={'use_reentrant': False},\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0005,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./nature-buddy/runs/Nov07_22-40-20_ip-10-192-12-69,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=3,\n",
      "no_cuda=False,\n",
      "num_train_epochs=8,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./nature-buddy,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=True,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./nature-buddy,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=0,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# Setup logging\n",
    "###############\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "log_level = train_conf.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "logger.info(f\"Training/evaluation parameters {train_conf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91712ecb630341f29d53d7b370d3a6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/704 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-11-07 22:41:01,872 >> loading configuration file config.json from cache at /home/zeus/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/28e66ca6931668447a3bac213f23d990ad3b0e2b/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-11-07 22:41:01,875 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"HuggingFaceTB/SmolLM2-135M\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 576,\n",
      "  \"initializer_range\": 0.041666666666666664,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"is_llama_config\": true,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 9,\n",
      "  \"num_hidden_layers\": 30,\n",
      "  \"num_key_value_heads\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_interleaved\": false,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 49152\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0977b1c2533f42d188e050f48f4dc1e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3678] 2024-11-07 22:41:27,852 >> loading weights file model.safetensors from cache at /home/zeus/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/28e66ca6931668447a3bac213f23d990ad3b0e2b/model.safetensors\n",
      "[INFO|modeling_utils.py:1606] 2024-11-07 22:41:27,870 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1038] 2024-11-07 22:41:27,872 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4507] 2024-11-07 22:41:28,342 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4515] 2024-11-07 22:41:28,343 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at HuggingFaceTB/SmolLM2-135M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c059e60fadd4823928701289479e093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:993] 2024-11-07 22:41:28,411 >> loading configuration file generation_config.json from cache at /home/zeus/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/28e66ca6931668447a3bac213f23d990ad3b0e2b/generation_config.json\n",
      "[INFO|configuration_utils.py:1038] 2024-11-07 22:41:28,411 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-11-07 22:41:28,758 >> loading file vocab.json from cache at /home/zeus/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/5a33ba103645800d7b3790c4448546c1b73efc71/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-11-07 22:41:28,758 >> loading file merges.txt from cache at /home/zeus/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/5a33ba103645800d7b3790c4448546c1b73efc71/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-11-07 22:41:28,759 >> loading file tokenizer.json from cache at /home/zeus/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/5a33ba103645800d7b3790c4448546c1b73efc71/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-11-07 22:41:28,760 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-11-07 22:41:28,760 >> loading file special_tokens_map.json from cache at /home/zeus/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/5a33ba103645800d7b3790c4448546c1b73efc71/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-11-07 22:41:28,760 >> loading file tokenizer_config.json from cache at /home/zeus/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/5a33ba103645800d7b3790c4448546c1b73efc71/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "####################\n",
    "# Base Model Loading\n",
    "####################\n",
    "checkpoint_path = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model_kwargs = dict(\n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    "#    attn_implementation=\"flash_attention_2\",  # only works on latest gpus, probably not worth it in most cases\n",
    "     torch_dtype=torch.bfloat16,\n",
    "   device_map='auto'\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_path, **model_kwargs)\n",
    "\n",
    "\n",
    "###################\n",
    "# Tokenizer Loading\n",
    "###################\n",
    "\n",
    "checkpoint_path = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "tokenizer.model_max_length = 2048\n",
    "tokenizer.pad_token = \"<|endoftext|>\"  # note this is specific to smollm\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "tokenizer.padding_side = 'right'\n",
    "# https://stackoverflow.com/questions/76446228/setting-padding-token-as-eos-token-when-using-datacollatorforlanguagemodeling-fr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the fine-tune \n",
    "\n",
    "Now that the synthetic dataset is made, next up is ensure the model is capable of answering like we expect, without the large system prompt impacting latency. \n",
    "\n",
    "The solution to this is to open up the dataset, replace the system prompt with something much simpler, and starting training with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:48 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:48 - INFO - datasets.info - Loading Dataset info from /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset nature_buddy_sft (/home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:48 - INFO - datasets.builder - Found cached dataset nature_buddy_sft (/home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:48 - INFO - datasets.info - Loading Dataset info from /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/zeus/.cache/huggingface/datasets/HuggingFaceTB___everyday-conversations-llama3.1-2k/default/0.0.0/451e129a6730488b7213951eb815af95f381eeea\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.info - Loading Dataset info from /home/zeus/.cache/huggingface/datasets/HuggingFaceTB___everyday-conversations-llama3.1-2k/default/0.0.0/451e129a6730488b7213951eb815af95f381eeea\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset everyday-conversations-llama3.1-2k (/home/zeus/.cache/huggingface/datasets/HuggingFaceTB___everyday-conversations-llama3.1-2k/default/0.0.0/451e129a6730488b7213951eb815af95f381eeea)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.builder - Found cached dataset everyday-conversations-llama3.1-2k (/home/zeus/.cache/huggingface/datasets/HuggingFaceTB___everyday-conversations-llama3.1-2k/default/0.0.0/451e129a6730488b7213951eb815af95f381eeea)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/zeus/.cache/huggingface/datasets/HuggingFaceTB___everyday-conversations-llama3.1-2k/default/0.0.0/451e129a6730488b7213951eb815af95f381eeea\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.info - Loading Dataset info from /home/zeus/.cache/huggingface/datasets/HuggingFaceTB___everyday-conversations-llama3.1-2k/default/0.0.0/451e129a6730488b7213951eb815af95f381eeea\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f6ef5175974359a9bab4729b466699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-415bb2ac2f21f986.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-415bb2ac2f21f986.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some of the datasets have disparate format. Resetting the format of the concatenated dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.arrow_dataset - Some of the datasets have disparate format. Resetting the format of the concatenated dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #0 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00000_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.arrow_dataset - Process #0 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00000_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #1 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00001_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.arrow_dataset - Process #1 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00001_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #2 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00002_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.arrow_dataset - Process #2 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00002_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #3 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00003_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.arrow_dataset - Process #3 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00003_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #4 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00004_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.arrow_dataset - Process #4 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00004_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #5 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00005_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.arrow_dataset - Process #5 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00005_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #6 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00006_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.arrow_dataset - Process #6 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00006_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #7 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00007_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.arrow_dataset - Process #7 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00007_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #8 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00008_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.arrow_dataset - Process #8 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00008_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #9 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00009_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.arrow_dataset - Process #9 will write at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00009_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spawning 10 processes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.arrow_dataset - Spawning 10 processes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dde23f9d6b647b1bb936343ecae3a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train_sft (num_proc=10):   0%|          | 0/5078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00000_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00000_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00001_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00001_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00002_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00002_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00003_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00003_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00004_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00004_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00005_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00005_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00006_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00006_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00007_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00007_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00008_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00008_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00009_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-7e9bd9a539945fed_00009_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concatenating 10 shards\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:50 - INFO - datasets.arrow_dataset - Concatenating 10 shards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching indices mapping at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-5b81e0e5fc7b6cef.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:49:50 - INFO - datasets.arrow_dataset - Caching indices mapping at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-5b81e0e5fc7b6cef.arrow\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# Data Processing\n",
    "##################\n",
    "def apply_chat_template(\n",
    "    example,\n",
    "    tokenizer,\n",
    "):\n",
    "\n",
    "    try:\n",
    "        messages = example[\"messages\"]\n",
    "\n",
    "        example[\"text\"] = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=False).strip('\\n')\n",
    "        return example\n",
    "    except Exception as e:\n",
    "        print(messages)\n",
    "\n",
    "raw_dataset = load_dataset(\"nkasmanoff/nature_buddy_sft\") \n",
    "\n",
    "\n",
    "raw_datset2 = load_dataset(\"HuggingFaceTB/everyday-conversations-llama3.1-2k\")\n",
    "train_dataset = raw_dataset['train']\n",
    "# delete any rows from train_dataset where the messages are empty\n",
    "train_dataset = train_dataset.filter(lambda x: len(x['messages']) > 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset2 = raw_datset2['train_sft']\n",
    "train_dataset = datasets.concatenate_datasets([train_dataset, train_dataset2])\n",
    "column_names = list(train_dataset.features)\n",
    "\n",
    "processed_train_dataset = train_dataset.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    num_proc=10,\n",
    "    desc=\"Applying chat template to train_sft\",\n",
    ")\n",
    "\n",
    "# shuffle the dataset\n",
    "processed_train_dataset = processed_train_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5078"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_train_dataset )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\n",
      "I'm going on a wilderness survival trip and I'm not sure how to find<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "model.eval();\n",
    "#prompt = \"\"\"What is the oort cloud?\"\"\"\n",
    "prompt = \"I'm going on a wilderness survival trip and I'm not sure how to find food. Can you help me?\"\n",
    "#prompt = f\"<|im_start|>system\\nYou are Pi-Card, the Raspberry Pi voice assistant.<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "prompt = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "input_ids = input_ids.to(model.device)\n",
    "output = model.generate(input_ids, max_new_tokens=256,  do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=False, pad_token_id = tokenizer.eos_token_id)\n",
    "formatted_output_text = \"<|im_end|>\".join(output_text.split(\"<|im_end|>\")[:2]) + \"<|im_end|>\"\n",
    "print(formatted_output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "[INFO|training_args.py:2100] 2024-11-07 22:51:18,168 >> PyTorch: setting up devices\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11219f25cf2640a39d622cf2eafbcef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-73f8361304d3b2ba.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 22:51:18 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/zeus/.cache/huggingface/datasets/nkasmanoff___nature_buddy_sft/default/0.0.0/d5d11b39304ec8b237019b762793d8d8c61cbf9b/cache-73f8361304d3b2ba.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2134] 2024-11-07 22:51:20,860 >> ***** Running training *****\n",
      "[INFO|trainer.py:2135] 2024-11-07 22:51:20,861 >>   Num examples = 5,078\n",
      "[INFO|trainer.py:2136] 2024-11-07 22:51:20,861 >>   Num Epochs = 8\n",
      "[INFO|trainer.py:2137] 2024-11-07 22:51:20,862 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:2140] 2024-11-07 22:51:20,862 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2141] 2024-11-07 22:51:20,863 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2142] 2024-11-07 22:51:20,863 >>   Total optimization steps = 5,080\n",
      "[INFO|trainer.py:2143] 2024-11-07 22:51:20,864 >>   Number of trainable parameters = 134,515,008\n",
      "[INFO|integration_utils.py:807] 2024-11-07 22:51:20,865 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4283' max='5080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4283/5080 19:06 < 03:33, 3.73 it/s, Epoch 6.74/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.789000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.045300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.008800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.927400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.911800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.845800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.698300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.649300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.641100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.643200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.603400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.550700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.395700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.398400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.406500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.407000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.403100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.391100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.267500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.251500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.256700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.252800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.252200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.197800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.169300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.170400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.168600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.170800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.173400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.163200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.133600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.138300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.137700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.139400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.130600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.125500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.126900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3503] 2024-11-07 22:53:29,201 >> Saving model checkpoint to ./nature-buddy/checkpoint-500\n",
      "[INFO|configuration_utils.py:472] 2024-11-07 22:53:29,203 >> Configuration saved in ./nature-buddy/checkpoint-500/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-07 22:53:29,204 >> Configuration saved in ./nature-buddy/checkpoint-500/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-07 22:53:29,693 >> Model weights saved in ./nature-buddy/checkpoint-500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-07 22:53:29,695 >> tokenizer config file saved in ./nature-buddy/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-07 22:53:29,696 >> Special tokens file saved in ./nature-buddy/checkpoint-500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-07 22:53:30,795 >> tokenizer config file saved in ./nature-buddy/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-07 22:53:30,796 >> Special tokens file saved in ./nature-buddy/special_tokens_map.json\n",
      "[INFO|trainer.py:3503] 2024-11-07 22:55:41,848 >> Saving model checkpoint to ./nature-buddy/checkpoint-1000\n",
      "[INFO|configuration_utils.py:472] 2024-11-07 22:55:41,850 >> Configuration saved in ./nature-buddy/checkpoint-1000/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-07 22:55:41,851 >> Configuration saved in ./nature-buddy/checkpoint-1000/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-07 22:55:42,319 >> Model weights saved in ./nature-buddy/checkpoint-1000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-07 22:55:42,321 >> tokenizer config file saved in ./nature-buddy/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-07 22:55:42,322 >> Special tokens file saved in ./nature-buddy/checkpoint-1000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-07 22:55:43,607 >> tokenizer config file saved in ./nature-buddy/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-07 22:55:43,608 >> Special tokens file saved in ./nature-buddy/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-07 22:55:43,642 >> Deleting older checkpoint [nature-buddy/checkpoint-500] due to args.save_total_limit\n",
      "[INFO|trainer.py:3503] 2024-11-07 22:57:55,046 >> Saving model checkpoint to ./nature-buddy/checkpoint-1500\n",
      "[INFO|configuration_utils.py:472] 2024-11-07 22:57:55,048 >> Configuration saved in ./nature-buddy/checkpoint-1500/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-07 22:57:55,049 >> Configuration saved in ./nature-buddy/checkpoint-1500/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-07 22:57:55,527 >> Model weights saved in ./nature-buddy/checkpoint-1500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-07 22:57:55,529 >> tokenizer config file saved in ./nature-buddy/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-07 22:57:55,530 >> Special tokens file saved in ./nature-buddy/checkpoint-1500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-07 22:57:56,832 >> tokenizer config file saved in ./nature-buddy/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-07 22:57:56,834 >> Special tokens file saved in ./nature-buddy/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-07 22:57:56,867 >> Deleting older checkpoint [nature-buddy/checkpoint-1000] due to args.save_total_limit\n",
      "[INFO|trainer.py:3503] 2024-11-07 23:00:07,340 >> Saving model checkpoint to ./nature-buddy/checkpoint-2000\n",
      "[INFO|configuration_utils.py:472] 2024-11-07 23:00:07,342 >> Configuration saved in ./nature-buddy/checkpoint-2000/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-07 23:00:07,343 >> Configuration saved in ./nature-buddy/checkpoint-2000/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-07 23:00:07,808 >> Model weights saved in ./nature-buddy/checkpoint-2000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-07 23:00:07,810 >> tokenizer config file saved in ./nature-buddy/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-07 23:00:07,811 >> Special tokens file saved in ./nature-buddy/checkpoint-2000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-07 23:00:09,089 >> tokenizer config file saved in ./nature-buddy/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-07 23:00:09,090 >> Special tokens file saved in ./nature-buddy/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-07 23:00:09,123 >> Deleting older checkpoint [nature-buddy/checkpoint-1500] due to args.save_total_limit\n",
      "[INFO|trainer.py:3503] 2024-11-07 23:02:20,968 >> Saving model checkpoint to ./nature-buddy/checkpoint-2500\n",
      "[INFO|configuration_utils.py:472] 2024-11-07 23:02:20,970 >> Configuration saved in ./nature-buddy/checkpoint-2500/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-07 23:02:20,971 >> Configuration saved in ./nature-buddy/checkpoint-2500/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-07 23:02:21,456 >> Model weights saved in ./nature-buddy/checkpoint-2500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-07 23:02:21,458 >> tokenizer config file saved in ./nature-buddy/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-07 23:02:21,459 >> Special tokens file saved in ./nature-buddy/checkpoint-2500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-07 23:02:22,788 >> tokenizer config file saved in ./nature-buddy/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-07 23:02:22,789 >> Special tokens file saved in ./nature-buddy/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-07 23:02:22,828 >> Deleting older checkpoint [nature-buddy/checkpoint-2000] due to args.save_total_limit\n",
      "[INFO|trainer.py:3503] 2024-11-07 23:04:40,249 >> Saving model checkpoint to ./nature-buddy/checkpoint-3000\n",
      "[INFO|configuration_utils.py:472] 2024-11-07 23:04:40,251 >> Configuration saved in ./nature-buddy/checkpoint-3000/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-07 23:04:40,252 >> Configuration saved in ./nature-buddy/checkpoint-3000/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-07 23:04:40,714 >> Model weights saved in ./nature-buddy/checkpoint-3000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-07 23:04:40,716 >> tokenizer config file saved in ./nature-buddy/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-07 23:04:40,717 >> Special tokens file saved in ./nature-buddy/checkpoint-3000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-07 23:04:42,030 >> tokenizer config file saved in ./nature-buddy/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-07 23:04:42,031 >> Special tokens file saved in ./nature-buddy/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-07 23:04:42,065 >> Deleting older checkpoint [nature-buddy/checkpoint-2500] due to args.save_total_limit\n",
      "[INFO|trainer.py:3503] 2024-11-07 23:06:57,365 >> Saving model checkpoint to ./nature-buddy/checkpoint-3500\n",
      "[INFO|configuration_utils.py:472] 2024-11-07 23:06:57,367 >> Configuration saved in ./nature-buddy/checkpoint-3500/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-07 23:06:57,368 >> Configuration saved in ./nature-buddy/checkpoint-3500/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-07 23:06:57,830 >> Model weights saved in ./nature-buddy/checkpoint-3500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-07 23:06:57,832 >> tokenizer config file saved in ./nature-buddy/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-07 23:06:57,833 >> Special tokens file saved in ./nature-buddy/checkpoint-3500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-07 23:06:59,115 >> tokenizer config file saved in ./nature-buddy/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-07 23:06:59,116 >> Special tokens file saved in ./nature-buddy/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-07 23:06:59,150 >> Deleting older checkpoint [nature-buddy/checkpoint-3000] due to args.save_total_limit\n",
      "[INFO|trainer.py:3503] 2024-11-07 23:09:10,606 >> Saving model checkpoint to ./nature-buddy/checkpoint-4000\n",
      "[INFO|configuration_utils.py:472] 2024-11-07 23:09:10,608 >> Configuration saved in ./nature-buddy/checkpoint-4000/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-11-07 23:09:10,609 >> Configuration saved in ./nature-buddy/checkpoint-4000/generation_config.json\n",
      "[INFO|modeling_utils.py:2799] 2024-11-07 23:09:11,073 >> Model weights saved in ./nature-buddy/checkpoint-4000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-07 23:09:11,075 >> tokenizer config file saved in ./nature-buddy/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-07 23:09:11,075 >> Special tokens file saved in ./nature-buddy/checkpoint-4000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-11-07 23:09:12,417 >> tokenizer config file saved in ./nature-buddy/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-11-07 23:09:12,418 >> Special tokens file saved in ./nature-buddy/special_tokens_map.json\n",
      "[INFO|trainer.py:3595] 2024-11-07 23:09:12,452 >> Deleting older checkpoint [nature-buddy/checkpoint-3500] due to args.save_total_limit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 15\u001b[0m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain();\n\u001b[1;32m      6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m      7\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      8\u001b[0m     args\u001b[38;5;241m=\u001b[39mtrain_conf,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#packing=True,\u001b[39;00m\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m metrics \u001b[38;5;241m=\u001b[39m train_result\u001b[38;5;241m.\u001b[39mmetrics\n\u001b[1;32m     17\u001b[0m trainer\u001b[38;5;241m.\u001b[39mlog_metrics(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:450\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 450\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   1928\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 1929\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1930\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1931\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1936\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:2284\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###########\n",
    "# Training\n",
    "###########\n",
    "\n",
    "model.train();\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=train_conf,\n",
    "    train_dataset=processed_train_dataset,\n",
    "    max_seq_length=2048,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    #packing=True,\n",
    ")\n",
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the checkpoint\n",
    "\n",
    "# find most recently created folder in checkpoint_dir and set as checkpoint path\n",
    "checkpoint_path = sorted(os.listdir(train_conf.output_dir))[-1]\n",
    "checkpoint_path = os.path.join(train_conf.output_dir, checkpoint_path)\n",
    "model_kwargs = dict(\n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    "     torch_dtype=torch.bfloat16,\n",
    "   device_map='auto'\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_path, **model_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "#prompt = \"\"\"What is the oort cloud?\"\"\"\n",
    "prompt = \"What is the safest way to purify water in the wilderness?\"\n",
    "#prompt = f\"<|im_start|>system\\nYou are Pi-Card, the Raspberry Pi voice assistant.<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "prompt = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "input_ids = input_ids.to(model.device)\n",
    "output = model.generate(input_ids, max_new_tokens=256,  do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=False, pad_token_id = tokenizer.eos_token_id)\n",
    "formatted_output_text = \"<|im_end|>\".join(output_text.split(\"<|im_end|>\")[:2]) + \"<|im_end|>\" \n",
    "print(formatted_output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving to gguf\n",
    "#https://github.com/ggerganov/llama.cpp/discussions/2948\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by downloading llama-cpp if not already done\n",
    "\n",
    "#!git clone https://github.com/ggerganov/llama.cpp.git\n",
    "!pip install -r llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gguf file\n",
    "\n",
    "# Please note you'll need to update the checkpoint path and model names to the one you want to convert & save\n",
    "!python llama.cpp/convert_hf_to_gguf.py nature-buddy/checkpoint-2005 --outfile nature-buddy-0.135b-f16.gguf --outtype f16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quanitzation output is going to have an outsized impact on latency / performance. \n",
    "\n",
    "While f16 is the default and good, it's worth noting the model was trained using bf16, a slightly different format, so that outtype may be worth testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the gguf you can either work with that directly, or convert it to an ollama format, which can be easier to work with in some cases. \n",
    "\n",
    "For instructions on how to do this, please see the instructions in create ollama text file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "base_model_name = \"HuggingFaceTB/SmolLM2-360M\"\n",
    "chat_model_name = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "noah_model_name = \"nkasmanoff/picard\"\n",
    "\n",
    "model_kwargs = dict(\n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    "     torch_dtype=torch.bfloat16,\n",
    "   device_map='auto'\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name, **model_kwargs)\n",
    "chat_model = AutoModelForCausalLM.from_pretrained(chat_model_name, **model_kwargs)\n",
    "noah_model = AutoModelForCausalLM.from_pretrained(noah_model_name, **model_kwargs)\n",
    "\n",
    "\n",
    "checkpoint_path = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "tokenizer.model_max_length = 2048\n",
    "tokenizer.pad_token = \"<|endoftext|>\"  # note this is specific to smollm\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token )\n",
    "tokenizer.padding_side = 'right'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_weight_diff(base_weight, chat_weight):\n",
    "    return torch.abs(base_weight - chat_weight).mean().item()\n",
    "\n",
    "def calculate_layer_diffs(base_model, chat_model):\n",
    "    layer_diffs = []\n",
    "    for base_layer, chat_layer in zip(base_model.model.layers, chat_model.model.layers):\n",
    "        layer_diff = {\n",
    "            'input_layernorm': calculate_weight_diff(base_layer.input_layernorm.weight, chat_layer.input_layernorm.weight),\n",
    "            # 'mlp_down_proj': calculate_weight_diff(base_layer.mlp.down_proj.weight, chat_layer.mlp.down_proj.weight),\n",
    "            # 'mlp_gate_proj': calculate_weight_diff(base_layer.mlp.gate_proj.weight, chat_layer.mlp.gate_proj.weight),\n",
    "            # 'mlp_up_proj': calculate_weight_diff(base_layer.mlp.up_proj.weight, chat_layer.mlp.up_proj.weight),\n",
    "            'post_attention_layernorm': calculate_weight_diff(base_layer.post_attention_layernorm.weight, chat_layer.post_attention_layernorm.weight),\n",
    "            'self_attn_q_proj': calculate_weight_diff(base_layer.self_attn.q_proj.weight, chat_layer.self_attn.q_proj.weight),\n",
    "            'self_attn_k_proj': calculate_weight_diff(base_layer.self_attn.k_proj.weight, chat_layer.self_attn.k_proj.weight),\n",
    "            'self_attn_v_proj': calculate_weight_diff(base_layer.self_attn.v_proj.weight, chat_layer.self_attn.v_proj.weight),\n",
    "            'self_attn_o_proj': calculate_weight_diff(base_layer.self_attn.o_proj.weight, chat_layer.self_attn.o_proj.weight)\n",
    "        }\n",
    "        layer_diffs.append(layer_diff)\n",
    "    return layer_diffs\n",
    "\n",
    "def visualize_layer_diffs(layer_diffs):\n",
    "    num_layers = len(layer_diffs)\n",
    "    num_components = len(layer_diffs[0])\n",
    "    \n",
    "    fig, axs = plt.subplots(1, num_components, figsize=(24, 8))\n",
    "    fig.suptitle(f\"{base_model_name} <> {chat_model_name}\", fontsize=16)\n",
    "    \n",
    "    for i, component in enumerate(layer_diffs[0].keys()):\n",
    "        component_diffs = [[layer_diff[component]] for layer_diff in layer_diffs]\n",
    "        sns.heatmap(component_diffs, annot=True, fmt=\".6f\", cmap=\"YlGnBu\", ax=axs[i], cbar_kws={\"shrink\": 0.8})\n",
    "        axs[i].set_title(component)\n",
    "        axs[i].set_xlabel(\"Layer\")\n",
    "        axs[i].set_ylabel(\"Difference\")\n",
    "        axs[i].set_xticks([])\n",
    "        axs[i].set_yticks(range(num_layers))\n",
    "        axs[i].set_yticklabels(range(num_layers))\n",
    "        axs[i].invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer_diffs = calculate_layer_diffs(base_model, chat_model)\n",
    "\n",
    "visualize_layer_diffs(layer_diffs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer_diffs = calculate_layer_diffs(noah_model, chat_model)\n",
    "\n",
    "visualize_layer_diffs(layer_diffs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_residuals(base_model, chat_model, final_model):\n",
    "    \"\"\"\n",
    "    For every parameter in this model, calculate the residual between base_model and chat_model. \n",
    "    Add that residual value to final_model.\n",
    "    \"\"\"\n",
    "\n",
    "    for base_param, chat_param, final_param in zip(base_model.parameters(), chat_model.parameters(), final_model.parameters()):\n",
    "        residual = chat_param - base_param\n",
    "        final_param.data += 0.5 * residual\n",
    "\n",
    "    return final_model\n",
    "\n",
    "def average_weights(chat_model,final_model):\n",
    "    \"\"\"For every parameter in this model, calculate the average between chat_model and final_model.\n",
    "    \"\"\"\n",
    "\n",
    "    for chat_param, final_param in zip(chat_model.parameters(), final_model.parameters()):\n",
    "        avg = (chat_param + final_param) / 2\n",
    "        final_param.data = avg\n",
    "\n",
    "    return final_model\n",
    "    \n",
    "final_model = get_residuals(base_model, chat_model, noah_model)\n",
    "#final_model = average_weights(chat_model, noah_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calculate_layer_diffs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m noah_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(noah_model_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m----> 2\u001b[0m layer_diffs \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_layer_diffs\u001b[49m(noah_model, final_model)\n\u001b[1;32m      4\u001b[0m visualize_layer_diffs(layer_diffs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'calculate_layer_diffs' is not defined"
     ]
    }
   ],
   "source": [
    "noah_model = AutoModelForCausalLM.from_pretrained(noah_model_name, **model_kwargs)\n",
    "layer_diffs = calculate_layer_diffs(noah_model, final_model)\n",
    "\n",
    "visualize_layer_diffs(layer_diffs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save final model and tokenizer to disk\n",
    "checkpoint_path = \"picard-merged/combined-weights\"\n",
    "final_model.save_pretrained(checkpoint_path)\n",
    "tokenizer.save_pretrained(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Pi-Card.<|im_end|>\n",
      "<|im_start|>user\n",
      "Who is Sofie?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I'm Pi-Card, not Sofie. I'm a text-to-voice assistant hosted on a Raspberry Pi, here to help with various tasks and answer questions.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "model = final_model\n",
    "model.eval();\n",
    "system_prompt =\"<|im_start|>system\\nYou are Pi-Card.<|im_end|>\\n\"\n",
    "\n",
    "#prompt = \"\"\"I have 45 pills. Sofie dose is 1 pill in morning and half pill at night. How long will this last\"\"\"\n",
    "#prompt = \"What does your name stand for?\"\n",
    "prompt = \"Who is Sofie?\"\n",
    "#prompt = \"Who is your favorite cat\"\n",
    "#prompt = \"What is the square root of 144?\"\n",
    "#prompt = \"Who is your favorite star trek captain?\"\n",
    "prompt = f\"{system_prompt}<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "input_ids = input_ids.to(model.device)\n",
    "output = model.generate(input_ids, max_new_tokens=256,  do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=False, pad_token_id = tokenizer.eos_token_id)\n",
    "formatted_output_text = \"<|im_end|>\".join(output_text.split(\"<|im_end|>\")[:3]) + \"<|im_end|>\"\n",
    "print(formatted_output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = AutoModelForCausalLM.from_pretrained(chat_model_name, **model_kwargs)\n",
    "\n",
    "model = chat_model\n",
    "model.eval();\n",
    "system_prompt = \"\"#\"<|im_start|>system\\nYou are Pi-Card, the Raspberry Pi AI assistant.<|im_end|>\\n\"\n",
    "\n",
    "prompt = \"\"\"I have 45 pills. Sofie dose is 1 pill in morning and half pill at night. How long will this last\"\"\"\n",
    "#prompt = \"What should I do if I see a bear while camping?\"\n",
    "\n",
    "#prompt = \"Who is your favorite star trek captain?\"\n",
    "prompt = f\"{system_prompt}<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "input_ids = input_ids.to(model.device)\n",
    "output = model.generate(input_ids, max_new_tokens=256,  do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=False, pad_token_id = tokenizer.eos_token_id)\n",
    "formatted_output_text = \"<|im_end|>\".join(output_text.split(\"<|im_end|>\")[:3]) + \"<|im_end|>\"\n",
    "print(formatted_output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Pi-Card.<|im_end|>\n",
      "<|im_start|>user\n",
      "I have 45 pills. Sofie dose is 1 pill in morning and half pill at night. How long will this last<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Sofie's dosing schedule is typical for many people, where a single dose in the morning and then a smaller dose at night is common. If you take 45 pills, you would typically finish one full dose, which is 2 pills in the morning and 1 pill at night.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "#chat_model = AutoModelForCausalLM.from_pretrained(chat_model_name, **model_kwargs)\n",
    "noah_model = AutoModelForCausalLM.from_pretrained(noah_model_name, **model_kwargs)\n",
    "model = noah_model\n",
    "model.eval();\n",
    "system_prompt = \"<|im_start|>system\\nYou are Pi-Card.<|im_end|>\\n\"\n",
    "\n",
    "prompt = \"\"\"I have 45 pills. Sofie dose is 1 pill in morning and half pill at night. How long will this last\"\"\"\n",
    "#prompt = \"What should I do if I see a bear while camping?\"\n",
    "\n",
    "#prompt = \"Who is your favorite star trek captain?\"\n",
    "prompt = f\"{system_prompt}<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "input_ids = input_ids.to(model.device)\n",
    "output = model.generate(input_ids, max_new_tokens=256,  do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=False, pad_token_id = tokenizer.eos_token_id)\n",
    "formatted_output_text = \"<|im_end|>\".join(output_text.split(\"<|im_end|>\")[:3]) + \"<|im_end|>\"\n",
    "print(formatted_output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
